{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso, LassoCV, LinearRegression\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy.ma as ma\n",
    "from sklearn import base\n",
    "import warnings\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "path_train = \"data/train_data.csv\"\n",
    "path_test = \"data/test_data.csv\"\n",
    "target_ = \"contest-tmp2m-14d__tmp2m\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path_train)\n",
    "df_test = pd.read_csv(path_test)\n",
    "\n",
    "date_col = \"startdate\"\n",
    "\n",
    "df_train[date_col] = pd.to_datetime(df_train[date_col])\n",
    "df_test[date_col] = pd.to_datetime(df_test[date_col])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "   index  lat       lon  startdate  contest-pevpr-sfc-gauss-14d__pevpr  \\\n0      0  0.0  0.833333 2014-09-01                              237.00   \n1      1  0.0  0.833333 2014-09-02                              228.90   \n2      2  0.0  0.833333 2014-09-03                              220.69   \n3      3  0.0  0.833333 2014-09-04                              225.28   \n4      4  0.0  0.833333 2014-09-05                              237.24   \n\n   nmme0-tmp2m-34w__cancm30  nmme0-tmp2m-34w__cancm40  \\\n0                     29.02                     31.64   \n1                     29.02                     31.64   \n2                     29.02                     31.64   \n3                     29.02                     31.64   \n4                     29.02                     31.64   \n\n   nmme0-tmp2m-34w__ccsm30  nmme0-tmp2m-34w__ccsm40  nmme0-tmp2m-34w__cfsv20  \\\n0                    29.57                    30.73                    29.71   \n1                    29.57                    30.73                    29.71   \n2                    29.57                    30.73                    29.71   \n3                    29.57                    30.73                    29.71   \n4                    29.57                    30.73                    29.71   \n\n   ...  wind-vwnd-925-2010-11  wind-vwnd-925-2010-12  wind-vwnd-925-2010-13  \\\n0  ...                 -27.68                 -37.21                   8.32   \n1  ...                 -21.13                 -36.57                   8.77   \n2  ...                 -10.72                 -34.16                   6.99   \n3  ...                   0.33                 -31.04                   6.17   \n4  ...                   9.83                 -31.80                   7.47   \n\n   wind-vwnd-925-2010-14  wind-vwnd-925-2010-15  wind-vwnd-925-2010-16  \\\n0                   9.56                  -2.03                  48.13   \n1                  21.17                   4.44                  48.60   \n2                  32.16                   5.01                  48.53   \n3                  39.66                  -1.41                  50.59   \n4                  38.62                  -5.21                  54.73   \n\n   wind-vwnd-925-2010-17  wind-vwnd-925-2010-18  wind-vwnd-925-2010-19  \\\n0                  28.09                 -13.50                  11.90   \n1                  27.41                 -23.77                  15.44   \n2                  19.21                 -33.16                  15.11   \n3                   8.29                 -37.22                  18.24   \n4                  -2.58                 -42.30                  21.91   \n\n   wind-vwnd-925-2010-20  \n0                   4.58  \n1                   3.42  \n2                   4.82  \n3                   9.74  \n4                  10.95  \n\n[5 rows x 246 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>lat</th>\n      <th>lon</th>\n      <th>startdate</th>\n      <th>contest-pevpr-sfc-gauss-14d__pevpr</th>\n      <th>nmme0-tmp2m-34w__cancm30</th>\n      <th>nmme0-tmp2m-34w__cancm40</th>\n      <th>nmme0-tmp2m-34w__ccsm30</th>\n      <th>nmme0-tmp2m-34w__ccsm40</th>\n      <th>nmme0-tmp2m-34w__cfsv20</th>\n      <th>...</th>\n      <th>wind-vwnd-925-2010-11</th>\n      <th>wind-vwnd-925-2010-12</th>\n      <th>wind-vwnd-925-2010-13</th>\n      <th>wind-vwnd-925-2010-14</th>\n      <th>wind-vwnd-925-2010-15</th>\n      <th>wind-vwnd-925-2010-16</th>\n      <th>wind-vwnd-925-2010-17</th>\n      <th>wind-vwnd-925-2010-18</th>\n      <th>wind-vwnd-925-2010-19</th>\n      <th>wind-vwnd-925-2010-20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.833333</td>\n      <td>2014-09-01</td>\n      <td>237.00</td>\n      <td>29.02</td>\n      <td>31.64</td>\n      <td>29.57</td>\n      <td>30.73</td>\n      <td>29.71</td>\n      <td>...</td>\n      <td>-27.68</td>\n      <td>-37.21</td>\n      <td>8.32</td>\n      <td>9.56</td>\n      <td>-2.03</td>\n      <td>48.13</td>\n      <td>28.09</td>\n      <td>-13.50</td>\n      <td>11.90</td>\n      <td>4.58</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.833333</td>\n      <td>2014-09-02</td>\n      <td>228.90</td>\n      <td>29.02</td>\n      <td>31.64</td>\n      <td>29.57</td>\n      <td>30.73</td>\n      <td>29.71</td>\n      <td>...</td>\n      <td>-21.13</td>\n      <td>-36.57</td>\n      <td>8.77</td>\n      <td>21.17</td>\n      <td>4.44</td>\n      <td>48.60</td>\n      <td>27.41</td>\n      <td>-23.77</td>\n      <td>15.44</td>\n      <td>3.42</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>0.833333</td>\n      <td>2014-09-03</td>\n      <td>220.69</td>\n      <td>29.02</td>\n      <td>31.64</td>\n      <td>29.57</td>\n      <td>30.73</td>\n      <td>29.71</td>\n      <td>...</td>\n      <td>-10.72</td>\n      <td>-34.16</td>\n      <td>6.99</td>\n      <td>32.16</td>\n      <td>5.01</td>\n      <td>48.53</td>\n      <td>19.21</td>\n      <td>-33.16</td>\n      <td>15.11</td>\n      <td>4.82</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.833333</td>\n      <td>2014-09-04</td>\n      <td>225.28</td>\n      <td>29.02</td>\n      <td>31.64</td>\n      <td>29.57</td>\n      <td>30.73</td>\n      <td>29.71</td>\n      <td>...</td>\n      <td>0.33</td>\n      <td>-31.04</td>\n      <td>6.17</td>\n      <td>39.66</td>\n      <td>-1.41</td>\n      <td>50.59</td>\n      <td>8.29</td>\n      <td>-37.22</td>\n      <td>18.24</td>\n      <td>9.74</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.833333</td>\n      <td>2014-09-05</td>\n      <td>237.24</td>\n      <td>29.02</td>\n      <td>31.64</td>\n      <td>29.57</td>\n      <td>30.73</td>\n      <td>29.71</td>\n      <td>...</td>\n      <td>9.83</td>\n      <td>-31.80</td>\n      <td>7.47</td>\n      <td>38.62</td>\n      <td>-5.21</td>\n      <td>54.73</td>\n      <td>-2.58</td>\n      <td>-42.30</td>\n      <td>21.91</td>\n      <td>10.95</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 246 columns</p>\n</div>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fill null"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:04,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_na_cols(df):\n",
    "    count_na_df = df.isna().sum()\n",
    "    if count_na_df[count_na_df > 0].tolist():\n",
    "        return count_na_df[count_na_df > 0]\n",
    "    else:\n",
    "        return 'Clean dataset'\n",
    "\n",
    "col_na = [\n",
    "    'nmme0-tmp2m-34w__ccsm30',\n",
    "    'nmme-tmp2m-56w__ccsm3',\n",
    "    'nmme-prate-34w__ccsm3',\n",
    "    'nmme0-prate-56w__ccsm30',\n",
    "    'nmme0-prate-34w__ccsm30',\n",
    "    'nmme-prate-56w__ccsm3',\n",
    "    'nmme-tmp2m-34w__ccsm3']\n",
    "\n",
    "g_means =  ['nmme0-tmp2m-34w__nmme0mean',\n",
    " 'nmme-tmp2m-56w__nmmemean',\n",
    " 'nmme-prate-34w__nmmemean',\n",
    " 'nmme0-prate-56w__nmme0mean',\n",
    " 'nmme0-prate-34w__nmme0mean',\n",
    " 'nmme-prate-56w__nmmemean',\n",
    " 'nmme-tmp2m-34w__nmmemean']\n",
    "\n",
    "\n",
    "g_1 = ['nmme0-tmp2m-34w__cancm30',\n",
    "'nmme0-tmp2m-34w__cancm40',\n",
    "'nmme0-tmp2m-34w__ccsm40',\n",
    "'nmme0-tmp2m-34w__cfsv20',\n",
    "'nmme0-tmp2m-34w__gfdlflora0',\n",
    "'nmme0-tmp2m-34w__gfdlflorb0',\n",
    "'nmme0-tmp2m-34w__gfdl0',\n",
    "'nmme0-tmp2m-34w__nasa0']\n",
    "\n",
    "g_2 = ['nmme-tmp2m-56w__cancm3',\n",
    "'nmme-tmp2m-56w__cancm4',\n",
    "'nmme-tmp2m-56w__ccsm4',\n",
    "'nmme-tmp2m-56w__cfsv2',\n",
    "'nmme-tmp2m-56w__gfdl',\n",
    "'nmme-tmp2m-56w__gfdlflora',\n",
    "'nmme-tmp2m-56w__gfdlflorb',\n",
    "'nmme-tmp2m-56w__nasa']\n",
    "\n",
    "g_3 = ['nmme-prate-34w__cancm3',\n",
    "'nmme-prate-34w__cancm4',\n",
    "'nmme-prate-34w__ccsm4',\n",
    "'nmme-prate-34w__cfsv2',\n",
    "'nmme-prate-34w__gfdl',\n",
    "'nmme-prate-34w__gfdlflora',\n",
    "'nmme-prate-34w__gfdlflorb',\n",
    "'nmme-prate-34w__nasa']\n",
    "\n",
    "g_4 = [ 'nmme0-prate-56w__cancm30',\n",
    "'nmme0-prate-56w__cancm40',\n",
    "'nmme0-prate-56w__ccsm40',\n",
    "'nmme0-prate-56w__cfsv20',\n",
    "'nmme0-prate-56w__gfdlflora0',\n",
    "'nmme0-prate-56w__gfdlflorb0',\n",
    "'nmme0-prate-56w__gfdl0',\n",
    "'nmme0-prate-56w__nasa0']\n",
    "\n",
    "g_5 = ['nmme0-prate-34w__cancm30',\n",
    "'nmme0-prate-34w__cancm40',\n",
    "'nmme0-prate-34w__ccsm40',\n",
    "'nmme0-prate-34w__cfsv20',\n",
    "'nmme0-prate-34w__gfdlflora0',\n",
    "'nmme0-prate-34w__gfdlflorb0',\n",
    "'nmme0-prate-34w__gfdl0',\n",
    "'nmme0-prate-34w__nasa0']\n",
    "\n",
    "g_6 = ['nmme-prate-56w__cancm3',\n",
    "'nmme-prate-56w__cancm4',\n",
    "'nmme-prate-56w__ccsm4',\n",
    "'nmme-prate-56w__cfsv2',\n",
    "'nmme-prate-56w__gfdl',\n",
    "'nmme-prate-56w__gfdlflora',\n",
    "'nmme-prate-56w__gfdlflorb',\n",
    "'nmme-prate-56w__nasa']\n",
    "\n",
    "g_7 = ['nmme-tmp2m-34w__cancm3',\n",
    "'nmme-tmp2m-34w__cancm4',\n",
    "'nmme-tmp2m-34w__ccsm4',\n",
    "'nmme-tmp2m-34w__cfsv2',\n",
    "'nmme-tmp2m-34w__gfdl',\n",
    "'nmme-tmp2m-34w__gfdlflora',\n",
    "'nmme-tmp2m-34w__gfdlflorb',\n",
    "'nmme-tmp2m-34w__nasa']\n",
    "\n",
    "gs = [g_1, g_2, g_3, g_4, g_5, g_6, g_7]\n",
    "\n",
    "zip_cols = zip(col_na, gs, g_means)\n",
    "for c, g, m in tqdm(zip_cols):\n",
    "    df_train[c] = (df_train[m]*9) - df_train[g].sum(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "df_train.drop(columns='ccsm30', inplace=True)\n",
    "df_test.drop(columns='ccsm30', inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "target_ = 'contest-tmp2m-14d__tmp2m'\n",
    "\n",
    "col = [each for each in df_train.columns if \"contest\" in each and each != target_]\n",
    "\n",
    "def get_idx(lat, lon):\n",
    "    return str(round(lat, 4)) + \"_\" + str(round(lon, 4))\n",
    "\n",
    "df_train['idx'] = np.vectorize(get_idx)(df_train['lat'], df_train['lon'])\n",
    "df_test['idx'] = np.vectorize(get_idx)(df_test['lat'], df_test['lon'])\n",
    "\n",
    "for each in col:\n",
    "    df_train[each + \"_lag_1\"] = df_train.groupby(\"idx\")[each].shift(1).bfill()\n",
    "    df_test[each + \"_lag_1\"] = df_test.groupby(\"idx\")[each].shift(1).bfill()\n",
    "    df_train[each + \"_shift_1\"] = df_train.groupby(\"idx\")[each].shift(-1).ffill()\n",
    "    df_test[each + \"_shitf_1\"] = df_test.groupby(\"idx\")[each].shift(-1).ffill()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "bin_col = ['nmme-tmp2m-34w__nmmemean', 'nmme-tmp2m-56w__nmmemean', 'nmme0-tmp2m-34w__nmme0mean', 'nmme0mean']\n",
    "\n",
    "def bin_feature_tpm2m(x):\n",
    "    if x < -5:\n",
    "        return 'A'\n",
    "    elif x < 0:\n",
    "        return 'B'\n",
    "    elif x < 5:\n",
    "        return 'C'\n",
    "    elif x < 10:\n",
    "        return 'D'\n",
    "    elif x < 15:\n",
    "        return 'E'\n",
    "    elif x < 20:\n",
    "        return 'F'\n",
    "    elif x < 25:\n",
    "        return 'G'\n",
    "    else:\n",
    "        return 'H'\n",
    "\n",
    "for each in bin_col:\n",
    "    df_train[each + \"_bin\"] = np.vectorize(bin_feature_tpm2m)(df_train[each])\n",
    "    df_test[each + \"_bin\"] = np.vectorize(bin_feature_tpm2m)(df_test[each])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "imp_contest_cols = [\n",
    "    'contest-pevpr-sfc-gauss-14d__pevpr',\n",
    "    'contest-pres-sfc-gauss-14d__pres',\n",
    "    'contest-prwtr-eatm-14d__prwtr',\n",
    "    'contest-slp-14d__slp',\n",
    "    'contest-wind-h10-14d__wind-hgt-10',\n",
    "    'contest-wind-h100-14d__wind-hgt-100',\n",
    "    'contest-wind-h500-14d__wind-hgt-500',\n",
    "    'contest-wind-uwnd-250-14d__wind-uwnd-250',\n",
    "    'contest-wind-vwnd-925-14d__wind-vwnd-925'\n",
    "]\n",
    "\n",
    "def get_name_cols(suffix=''):\n",
    "    name_cols = [col + f'{suffix}' for col in imp_contest_cols]\n",
    "\n",
    "    return name_cols\n",
    "\n",
    "def feature_engineer(df, cols):\n",
    "\n",
    "    # df.loc[:, 'idx'] = np.vectorize(get_idx)(df['lat'], df['lon'])\n",
    "    df.loc[:, 'month'] = df['startdate'].dt.month\n",
    "    df.loc[:, 'year']  = df['startdate'].dt.year\n",
    "\n",
    "    mean_contest_cols = df.groupby(['idx', 'month', 'year'])[cols].transform(lambda x: x.mean()) \\\n",
    "        .rename(columns=dict(zip(cols, get_name_cols('_mean'))))\n",
    "    std_contest_cols = df.groupby(['idx', 'month', 'year'])[cols].transform(lambda x: x.std()) \\\n",
    "        .rename(columns=dict(zip(cols, get_name_cols('_std'))))\n",
    "    rolling_mean_contest_cols = df.groupby('idx')[cols].transform(lambda x: x.rolling(window=7, min_periods=3, win_type=\"triang\").mean()) \\\n",
    "        .rename(columns=dict(zip(cols, get_name_cols('_rolling_mean'))))\n",
    "    rolling_std_contest_cols = df.groupby('idx')[cols].transform(lambda x: x.rolling(window=7, min_periods=3).std()) \\\n",
    "        .rename(columns=dict(zip(cols, get_name_cols('_rolling_std'))))\n",
    "    expanding_mean_contest_cols = df.groupby('idx')[cols].transform(lambda x: x.expanding(2).mean()) \\\n",
    "        .rename(columns=dict(zip(cols, get_name_cols('_expanding_mean'))))\n",
    "    expanding_std_contest_cols = df.groupby('idx')[cols].transform(lambda x: x.expanding(2).std()) \\\n",
    "        .rename(columns=dict(zip(cols, get_name_cols('_expanding_std'))))\n",
    "    exponentially_weighted_average = df.groupby('idx')[cols].transform(lambda x: x.shift(1).ewm(alpha=0.95).mean()) \\\n",
    "        .rename(columns=dict(zip(cols, get_name_cols('_ewa'))))\n",
    "\n",
    "    df = pd.concat([df, mean_contest_cols, std_contest_cols, rolling_mean_contest_cols,\n",
    "                    rolling_std_contest_cols, expanding_mean_contest_cols,\n",
    "                    expanding_std_contest_cols, exponentially_weighted_average], axis=1)\n",
    "\n",
    "    df = df.fillna(method='bfill')\n",
    "    df.drop(columns=['month', 'year'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = feature_engineer(df_train, imp_contest_cols)\n",
    "df_test = feature_engineer(df_test, imp_contest_cols)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=[\"idx\"])\n",
    "df_test = df_test.drop(columns=[\"idx\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "def fourier_series(dates, period, series_order):\n",
    "    t = np.array(\n",
    "        (dates - datetime(1970, 1, 1))\n",
    "            .dt.total_seconds()\n",
    "            .astype(float)\n",
    "    ) / (3600 * 24.)\n",
    "\n",
    "    return np.column_stack([\n",
    "        fun((2.0 * (i + 1) * np.pi * t / period))\n",
    "        for i in range(series_order)\n",
    "        for fun in (np.sin, np.cos)\n",
    "    ])\n",
    "\n",
    "def make_seasonality_features(dates, period, series_order, prefix):\n",
    "    features = fourier_series(dates, period, series_order)\n",
    "    columns = [\n",
    "        '{}_delim_{}'.format(prefix, i + 1)\n",
    "        for i in range(features.shape[1])\n",
    "    ]\n",
    "    return pd.DataFrame(features, columns=columns)\n",
    "\n",
    "def fourier_transform(df):\n",
    "    def reconvert(df):\n",
    "        if not os.path.exists('delete_later'):\n",
    "            os.mkdir('delete_later')\n",
    "\n",
    "        df.to_csv('delete_later/df.csv')\n",
    "        df = pd.read_csv('delete_later/df.csv')\n",
    "\n",
    "        return df\n",
    "\n",
    "    if not os.path.exists('fourier'):\n",
    "        os.mkdir('fourier')\n",
    "\n",
    "    seasonalities = {\n",
    "        'monthly': {\n",
    "            'period': 30.5,\n",
    "            'fourier_order': 8,\n",
    "        }, 'quarterly': {\n",
    "            'period': 90,\n",
    "            'fourier_order': 10\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for name, props in seasonalities.items():\n",
    "\n",
    "        df['startdate']  = pd.to_datetime(df['startdate'])\n",
    "\n",
    "        t = np.array(\n",
    "            (df['startdate'] - datetime(1970, 1, 1))\n",
    "                .dt.total_seconds()\n",
    "                .astype(float)\n",
    "        ) / (3600 * 24.)\n",
    "\n",
    "        series_order = props['fourier_order']\n",
    "        period = props['period']\n",
    "\n",
    "        s = np.column_stack([\n",
    "                fun((2.0 * (i + 1) * np.pi * t / period))\n",
    "                for i in range(series_order)\n",
    "                for fun in (np.sin, np.cos)\n",
    "            ])\n",
    "\n",
    "        columns = [\n",
    "            '{}_delim_{}'.format(name, i + 1)\n",
    "            for i in range(s.shape[1])\n",
    "        ]\n",
    "\n",
    "        features = pd.DataFrame(s, columns=columns)\n",
    "\n",
    "        df = pd.concat([reconvert(df), reconvert(features)], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_idx_date(df, column_date, idx_name):\n",
    "    def get_idx(lat, lon):\n",
    "        return str(round(lat, 4)) + \"_\" + str(round(lon, 4))\n",
    "\n",
    "    df.loc[:, idx_name] = np.vectorize(get_idx)(df['lat'], df['lon'])\n",
    "    df.loc[:, column_date] = pd.to_datetime(df[column_date])\n",
    "    df.loc[:, 'month'] = df[column_date].dt.month\n",
    "    df.loc[:, 'year'] = df[column_date].dt.year\n",
    "\n",
    "    # df = fourier_transform(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_feature_train_data(df, column_date=\"startdate\", columns_cat = [], idx_name=\"idx\"):\n",
    "    df = handle_idx_date(df, column_date, idx_name)\n",
    "    df = df.drop(columns = [column_date])\n",
    "    columns_cat.append(idx_name)\n",
    "    list_lbEncoder = []\n",
    "\n",
    "    for each in columns_cat:\n",
    "        lbE = LabelEncoder().fit(df[each])\n",
    "        df[each] = lbE.transform(df[each])\n",
    "        list_lbEncoder.append(lbE)\n",
    "\n",
    "    df[columns_cat] = df[columns_cat].astype(\"category\")\n",
    "\n",
    "    return df, list_lbEncoder\n",
    "\n",
    "def handle_feature_test_data(df, lbEncoder, column_date=\"startdate\", columns_cat = [], idx_name=\"idx\"):\n",
    "    df = handle_idx_date(df, column_date, idx_name)\n",
    "    df = df.drop(columns = [column_date])\n",
    "    columns_cat.append(idx_name)\n",
    "    list_lbEncoder = []\n",
    "\n",
    "    for index, each in enumerate(columns_cat):\n",
    "        df[each] = lbEncoder[index].transform(df[each])\n",
    "\n",
    "    df[columns_cat] = df[columns_cat].astype(\"category\")\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "def reconvert(df):\n",
    "    if not os.path.exists('delete_later'):\n",
    "        os.mkdir('delete_later')\n",
    "\n",
    "    df.to_csv('delete_later/df.csv')\n",
    "    df = pd.read_csv('delete_later/df.csv')\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "def calculate_corr_target(X_train, X_val):\n",
    "    data_col = []\n",
    "    data_corr_train = []\n",
    "    data_corr_val = []\n",
    "\n",
    "    for col in X_train.columns:\n",
    "        corr_train = X_train['contest-tmp2m-14d__tmp2m'].corr(X_train[col])\n",
    "        corr_val = X_val['contest-tmp2m-14d__tmp2m'].corr(X_val[col])\n",
    "\n",
    "        data_col.append(col)\n",
    "        data_corr_train.append(corr_train)\n",
    "        data_corr_val.append(corr_val)\n",
    "\n",
    "    corr = pd.DataFrame(data={'col': data_col, 'corr_train': data_corr_train, 'corr_val': data_corr_val})\n",
    "\n",
    "    return corr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "corr_df = pd.read_csv(\"data/correlations_with_target.csv\") \\\n",
    "            .drop(columns='Unnamed: 0')\n",
    "corr_df = corr_df[(corr_df['corr'] >= 0.7)  | (corr_df['corr'] <= -0.7)]\n",
    "drop_col = corr_df[\"col\"].values\n",
    "drop_col = [each for each in drop_col if \"contest\" not in each and \"wind\" not in each and each != 'ccsm30']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LGBM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "def bayes_tunning():\n",
    "    def CB_opt(max_depth, num_leaves, colsample_bytree, reg_alpha,\n",
    "               reg_lambda, subsample, feature_fraction_bynode):\n",
    "\n",
    "        lgb = LGBMRegressor(\n",
    "            max_depth = round(max_depth),\n",
    "            num_leaves=round(num_leaves),\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            subsample=subsample,\n",
    "            feature_fraction_bynode=feature_fraction_bynode,\n",
    "            verbose=0,\n",
    "            n_estimators = 7999,\n",
    "            metric=\"rmse\",\n",
    "            boosting_type=\"dart\",\n",
    "            learning_rate=0.05,\n",
    "            n_jobs=-1,\n",
    "            extra_trees=True\n",
    "        )\n",
    "\n",
    "        lgb.fit(X_train, y_train, eval_metric=\"rmse\",\n",
    "                categorical_feature=[\n",
    "                    'climateregions__climateregion', 'idx', 'mjo1d__phase',\n",
    "                    'nmme-tmp2m-34w__nmmemean_bin', 'nmme-tmp2m-56w__nmmemean_bin',\n",
    "                    'nmme0-tmp2m-34w__nmme0mean_bin', 'nmme0mean_bin'\n",
    "                ]\n",
    "                )\n",
    "        ypred_valid = lgb.predict(X_valid)\n",
    "\n",
    "        return 1 / mean_squared_error(y_valid, ypred_valid, squared=False)\n",
    "\n",
    "    pbounds = {\n",
    "        'max_depth': (8, 17),\n",
    "        'num_leaves': (15, 35),\n",
    "        'colsample_bytree': (0.5, 0.8),\n",
    "        'reg_alpha': (0.1, 3),\n",
    "        'reg_lambda': (0.1, 3),\n",
    "        # 'min_split_gain': (0.001, 0.3),\n",
    "        'subsample': (0.5, 0.85),\n",
    "        'feature_fraction_bynode': (0.6, 0.85)\n",
    "    }\n",
    "\n",
    "    if not os.path.exists('Params_tunning/Log'):\n",
    "        os.makedirs('Params_tunning/Log')\n",
    "\n",
    "    optimizer = BayesianOptimization(f = CB_opt, pbounds = pbounds, verbose = 2, random_state = 209)\n",
    "    logger = JSONLogger(path=\"Params_tunning/Log/logs_{}.json\".format(index))\n",
    "    optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "    optimizer.maximize(init_points = 5, n_iter = 15)\n",
    "\n",
    "    print(optimizer.max['target'])\n",
    "\n",
    "    max_bo_params = optimizer.max['params']\n",
    "\n",
    "    return max_bo_params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "def get_bo_result(max_bo_params):\n",
    "    opt_params = {\n",
    "        'max_depth': round(max_bo_params['max_depth']),\n",
    "        'num_leaves': round(max_bo_params['num_leaves']),\n",
    "        'colsample_bytree': max_bo_params['colsample_bytree'],\n",
    "        'reg_alpha': max_bo_params['reg_alpha'],\n",
    "        'reg_lambda': max_bo_params['reg_lambda'],\n",
    "        'subsample': max_bo_params['subsample'],\n",
    "        'feature_fraction_bynode': max_bo_params['feature_fraction_bynode'],\n",
    "        'verbose': 0,\n",
    "        'n_estimators': 7999,\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'dart',\n",
    "        'learning_rate': 0.05,\n",
    "        'n_jobs': -1,\n",
    "        'extra_trees': True\n",
    "    }\n",
    "\n",
    "    return opt_params\n",
    "\n",
    "def get_best_params(path):\n",
    "    tmp = pd.read_json(path, lines=True)\n",
    "    max_t = 0\n",
    "    max_i = 0\n",
    "\n",
    "    for i in range(tmp.shape[0]):\n",
    "        line = tmp.loc[i, :]\n",
    "        t = line['target']\n",
    "\n",
    "        if t > max_t:\n",
    "            max_t = t\n",
    "            max_i = i\n",
    "\n",
    "    max_line = tmp.loc[max_i, :]\n",
    "    max_param = max_line['params']\n",
    "    max_param['max_depth'] = round(max_param['max_depth'])\n",
    "    max_param['num_leaves'] = round(max_param['num_leaves'])\n",
    "\n",
    "    return max_param"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - 0\n",
      "Test on month 1 and 2, year 2015\n",
      "Pre-processing...\n",
      "Tuning...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Take time:  1461.7198147773743\n",
      "Train_score: 0.275786395655264  Valid_score: 1.6154400118540062\n",
      "-------------\n",
      "Training - 1\n",
      "Test on month 3 and 4, year 2015\n",
      "Pre-processing...\n",
      "Tuning...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Take time:  1262.7144601345062\n",
      "Train_score: 0.2870544146413979  Valid_score: 1.2893031441615184\n",
      "-------------\n",
      "Training - 2\n",
      "Test on month 5 and 6, year 2015\n",
      "Pre-processing...\n",
      "Tuning...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Take time:  1430.2488820552826\n",
      "Train_score: 0.28845329143048987  Valid_score: 0.8235127411238644\n",
      "-------------\n",
      "Training - 3\n",
      "Test on month 7 and 8, year 2015\n",
      "Pre-processing...\n",
      "Tuning...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Take time:  1309.6921029090881\n",
      "Train_score: 0.28219984626614303  Valid_score: 0.6712081612942136\n",
      "-------------\n",
      "Training - 4\n",
      "Test on month 9 and 10, year 2015\n",
      "Pre-processing...\n",
      "Tuning...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Take time:  1244.6235830783844\n",
      "Train_score: 0.28921989292087114  Valid_score: 0.6769574450461665\n",
      "-------------\n",
      "Training - 5\n",
      "Test on month 11 and 12, year 2015\n",
      "Pre-processing...\n",
      "Tuning...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Take time:  1227.4460179805756\n",
      "Train_score: 0.2802800036428324  Valid_score: 1.7369557803703874\n",
      "-------------\n",
      "Training - 6\n",
      "Test on month 1 and 2, year 2016\n",
      "Pre-processing...\n",
      "Tuning...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Take time:  1208.3615696430206\n",
      "Train_score: 0.2732798283993581  Valid_score: 1.2937278937498786\n",
      "-------------\n",
      "Training - 7\n",
      "Test on month 3 and 4, year 2016\n",
      "Pre-processing...\n",
      "Tuning...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Take time:  1177.076651096344\n",
      "Train_score: 0.2854515956608626  Valid_score: 1.3904061112104926\n",
      "-------------\n",
      "Training - 8\n",
      "Test on month 5 and 6, year 2016\n",
      "Pre-processing...\n",
      "Tuning...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Take time:  1213.1789450645447\n",
      "Train_score: 0.2872238680428999  Valid_score: 0.7440849085295251\n",
      "-------------\n",
      "Training - 9\n",
      "Test on month 7 and 8, year 2016\n",
      "Pre-processing...\n",
      "Tuning...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Take time:  1219.0317070484161\n",
      "Train_score: 0.282775408800727  Valid_score: 0.5999367688030801\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "dict_result = {}\n",
    "\n",
    "year_valid = [2015, 2015, 2015, 2015, 2015, 2015, 2016, 2016, 2016, 2016]\n",
    "month_valid = [1, 3, 5, 7, 9, 11, 1, 3, 5, 7]\n",
    "\n",
    "# year_valid = [2015]\n",
    "# month_valid = [9]\n",
    "\n",
    "for index in range(len(year_valid)):\n",
    "    t = time.time()\n",
    "    y, m_s, m_e = year_valid[index], month_valid[index], month_valid[index] + 1\n",
    "\n",
    "    print(\"Training - {}\".format(index))\n",
    "    print(f'Test on month {m_s} and {m_e}, year {y}')\n",
    "\n",
    "    if index not in dict_result:\n",
    "\n",
    "        print('Pre-processing...')\n",
    "\n",
    "        X_valid = df_train[(df_train[date_col].dt.year == y) & (df_train[date_col].dt.month >= m_s) & (df_train[date_col].dt.month <= m_e)]\n",
    "        X_train = df_train[~((df_train[date_col].dt.year == y) & (df_train[date_col].dt.month >= m_s) & (df_train[date_col].dt.month <= m_e))]\n",
    "\n",
    "        y_train = X_train[target_]\n",
    "        y_valid = X_valid[target_]\n",
    "\n",
    "        date = np.array(X_valid['startdate'])\n",
    "\n",
    "        test_index = df_test[\"index\"].values\n",
    "        X_test = df_test.copy()\n",
    "\n",
    "        cat_cols = [i for i in X_train.select_dtypes(include='object').columns if i != date_col]\n",
    "        X_train, listEncoder = handle_feature_train_data(X_train, date_col, cat_cols.copy())\n",
    "        X_valid = handle_feature_test_data(X_valid, listEncoder, columns_cat=cat_cols.copy())\n",
    "        X_test = handle_feature_test_data(X_test, listEncoder, columns_cat=cat_cols.copy())\n",
    "\n",
    "        drop_ = [*drop_col, *[col for col in X_train.columns if 'Unnamed' in col], *['month', 'year']]\n",
    "        # drop_ = [col for col in X_train.columns if 'Unnamed' in col]\n",
    "        X_train = X_train.drop(columns=drop_)\n",
    "        X_valid = X_valid.drop(columns=drop_)\n",
    "        X_test = X_test.drop(columns=drop_)\n",
    "\n",
    "        X_train = X_train.drop(columns=target_)\n",
    "        X_valid = X_valid.drop(columns=target_)\n",
    "\n",
    "        # Model\n",
    "        print('Tuning...')\n",
    "\n",
    "        # max_bo_params = bayes_tunning()\n",
    "        # opt_params = get_bo_result(max_bo_params)\n",
    "\n",
    "        opt_params = {\n",
    "            \"colsample_bytree\": 0.52711076304943,\n",
    "            \"feature_fraction_bynode\": 0.8113186525184517,\n",
    "            \"max_depth\": round(11.243615300071312),\n",
    "            \"num_leaves\": round(23.514506247035474),\n",
    "            \"reg_alpha\": 2.784515861336046,\n",
    "            \"reg_lambda\": 0.8701389388373701,\n",
    "            \"subsample\": 0.5116088328800932,\n",
    "            'verbose': 0,\n",
    "            'n_estimators': 7999,\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'dart',\n",
    "            'learning_rate': 0.05,\n",
    "            'n_jobs': -1,\n",
    "            'extra_trees': True\n",
    "        }\n",
    "\n",
    "        reg = LGBMRegressor(**opt_params)\n",
    "\n",
    "        reg.fit(X_train, y_train,\n",
    "                eval_metric='rmse',\n",
    "                categorical_feature=[\n",
    "                    'climateregions__climateregion', 'idx', 'mjo1d__phase',\n",
    "                    'nmme-tmp2m-34w__nmmemean_bin', 'nmme-tmp2m-56w__nmmemean_bin',\n",
    "                    'nmme0-tmp2m-34w__nmme0mean_bin', 'nmme0mean_bin'\n",
    "                ])\n",
    "\n",
    "        ypred_train = reg.predict(X_train)\n",
    "        ypred_valid = reg.predict(X_valid)\n",
    "\n",
    "        result_train = mean_squared_error(y_train, ypred_train, squared=False)\n",
    "        result_valid = mean_squared_error(y_valid, ypred_valid, squared=False)\n",
    "\n",
    "        print(\"Take time: \", time.time() - t)\n",
    "        print(\"Train_score: {}  Valid_score: {}\".format(result_train, result_valid))\n",
    "\n",
    "        ypred_test = reg.predict(X_test)\n",
    "        dict_result[index] = ypred_test\n",
    "\n",
    "    else:\n",
    "        print('Fold was trained!')\n",
    "\n",
    "    print(\"-------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "ypred_test = np.mean([value for key, value in dict_result.items()], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "pd.DataFrame(data = {\"{}\".format(target_): ypred_test, \"index\": test_index}).to_csv(\"submit.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   col    imp\n299                                                idx  36750\n232                       nmme-tmp2m-34w__nmmemean_bin   7170\n57                       climateregions__climateregion   6574\n233                       nmme-tmp2m-56w__nmmemean_bin   6311\n213                       contest-slp-14d__slp_shift_1   5081\n..                                                 ...    ...\n78                               wind-vwnd-250-2010-20     26\n203                              wind-vwnd-925-2010-20     25\n264       contest-pres-sfc-gauss-14d__pres_rolling_std     25\n271  contest-wind-vwnd-925-14d__wind-vwnd-925_rolli...     18\n103                                           mei__nip      0\n\n[300 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>col</th>\n      <th>imp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>299</th>\n      <td>idx</td>\n      <td>36750</td>\n    </tr>\n    <tr>\n      <th>232</th>\n      <td>nmme-tmp2m-34w__nmmemean_bin</td>\n      <td>7170</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>climateregions__climateregion</td>\n      <td>6574</td>\n    </tr>\n    <tr>\n      <th>233</th>\n      <td>nmme-tmp2m-56w__nmmemean_bin</td>\n      <td>6311</td>\n    </tr>\n    <tr>\n      <th>213</th>\n      <td>contest-slp-14d__slp_shift_1</td>\n      <td>5081</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>wind-vwnd-250-2010-20</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>wind-vwnd-925-2010-20</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>264</th>\n      <td>contest-pres-sfc-gauss-14d__pres_rolling_std</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>271</th>\n      <td>contest-wind-vwnd-925-14d__wind-vwnd-925_rolli...</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>mei__nip</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = reg.feature_importances_\n",
    "\n",
    "data = {'col': X_train.columns, 'imp': importances}\n",
    "\n",
    "ft_imp_df = pd.DataFrame(data)\n",
    "ft_imp_df.sort_values('imp', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "ft_imp_df.to_csv('ft_imp.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "   contest-tmp2m-14d__tmp2m   index\n0                 28.608565  375734\n1                 28.622350  375735\n2                 28.708710  375736\n3                 28.752598  375737\n4                 28.785682  375738",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contest-tmp2m-14d__tmp2m</th>\n      <th>index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28.608565</td>\n      <td>375734</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>28.622350</td>\n      <td>375735</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>28.708710</td>\n      <td>375736</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>28.752598</td>\n      <td>375737</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28.785682</td>\n      <td>375738</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_tunning_1 = pd.read_csv('submit_tunning_1.csv')\n",
    "\n",
    "submit_lgbm = pd.read_csv('submit.csv')\n",
    "\n",
    "submit_lgbm.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "submit_ensemple = pd.DataFrame()\n",
    "\n",
    "submit_ensemple[target_] = 0.3 * submit_lgbm[target_] + 0.7 * submit_tunning_1[target_]\n",
    "submit_ensemple['index'] = submit_lgbm['index']\n",
    "\n",
    "submit_ensemple.to_csv('submit_ensemble.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TargetEncoder(base.BaseEstimator,base.TransformerMixin):\n",
    "    def __init__(self, colnames, targetName, colSupport=None,\n",
    "                  discardOriginal_col=False):\n",
    "        self.colnames = colnames\n",
    "        self.colSupport = colSupport\n",
    "        self.targetName = targetName\n",
    "        self.discardOriginal_col = discardOriginal_col\n",
    "        self.new_cols = colnames + '_tar'\n",
    "        self.data_transform = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.colSupport != None:\n",
    "            self.data_transform = X.groupby([self.colnames, self.colSupport]).agg({self.targetName: 'mean'}).rename(columns={self.targetName: self.new_cols})\n",
    "        else:\n",
    "            self.data_transform = X.groupby([self.colnames]).agg({self.targetName: 'mean'}).rename(columns={self.targetName: self.new_cols})\n",
    "        return self\n",
    "\n",
    "    def transform(self, X,train=True):\n",
    "        X = X.merge(self.data_transform.reset_index())\n",
    "        encoded_feature = X[self.new_cols].values\n",
    "        if train:\n",
    "            print('Correlation between the new feature, {} and, {} is {}.'.format(self.new_cols,self.targetName,\n",
    "                   ma.corrcoef(ma.masked_invalid(X[self.targetName].values), ma.masked_invalid(encoded_feature))[0][1]))\n",
    "        if self.discardOriginal_col:\n",
    "            X = X.drop(self.targetName, axis=1)\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dict_result = {}\n",
    "year_valid = [2015, 2015, 2015, 2015, 2015, 2015, 2016, 2016, 2016, 2016]\n",
    "month_valid = [1, 3, 5, 7, 9, 11, 1, 3, 5, 7]\n",
    "gap = 1\n",
    "\n",
    "for index in range(len(year_valid)):\n",
    "    if index in dict_result:\n",
    "        continue\n",
    "    y, m_s, m_e = year_valid[index], month_valid[index], month_valid[index] + 1\n",
    "    X_valid = df_train[(df_train[date_col].dt.year == y) & (df_train[date_col].dt.month >= m_s) & (df_train[date_col].dt.month <= m_e)]\n",
    "    X_train = df_train[~((df_train[date_col].dt.year == y) & (df_train[date_col].dt.month >= m_s) & (df_train[date_col].dt.month <= m_e))]\n",
    "\n",
    "    # y_train = X_train[target_].copy()\n",
    "    X_train = X_train.drop(columns=[\"index\"])\n",
    "    # y_valid = X_valid[target_].copy()\n",
    "    X_valid = X_valid.drop(columns=[\"index\"])\n",
    "\n",
    "    test_index = df_test[\"index\"].values\n",
    "    X_test = df_test.drop(columns=[\"index\"]).copy()\n",
    "\n",
    "    cat_cols = [i for i in X_train.select_dtypes(include='object').columns if i != date_col]\n",
    "    X_train, listEncoder, df_mean = handle_feature_train_data(X_train, date_col, cat_cols.copy())\n",
    "    X_valid = handle_feature_test_data(X_valid, listEncoder, df_mean, columns_cat=cat_cols.copy())\n",
    "    X_test = handle_feature_test_data(X_test, listEncoder, df_mean, columns_cat=cat_cols.copy())\n",
    "\n",
    "    target_climate = TargetEncoder(colnames='climateregions__climateregion', targetName=target_).fit(X_train)\n",
    "    target_idx = TargetEncoder(colnames='idx', targetName=target_).fit(X_train)\n",
    "\n",
    "    X_train = target_climate.transform(X_train)\n",
    "    X_valid = target_climate.transform(X_valid)\n",
    "    X_test = target_climate.transform(X_test, train=False)\n",
    "    X_train = target_idx.transform(X_train)\n",
    "    X_valid = target_idx.transform(X_valid)\n",
    "    X_test = target_idx.transform(X_test, train=False)\n",
    "\n",
    "    y_train = X_train[target_].copy()\n",
    "    y_valid = X_valid[target_].copy()\n",
    "    X_train = X_train.drop(columns=[target_])\n",
    "    X_valid = X_valid.drop(columns=[target_])\n",
    "\n",
    "    drop_ = [*drop_col, *[\"mei__nip\", \"idx\", \"climateregions__climateregion\", 'month', 'year'], *[col for col in X_train.columns if 'Unnamed' in col]]\n",
    "    # drop_ = [\"month\", \"day_of_year\", \"day_of_year_sin\", \"day_of_year_cos\", \"month_sin\", \"month_cos\", \"mei__nip\"]\n",
    "\n",
    "    X_train_ = X_train.drop(columns=drop_)\n",
    "    X_valid_ = X_valid.drop(columns=drop_)\n",
    "    X_test_ = X_test.drop(columns=drop_)\n",
    "\n",
    "    print(\"Training - {}\".format(index))\n",
    "\n",
    "    t = time.time()\n",
    "    lgb = LinearRegression(normalize=True)\n",
    "    lgb.fit(X_train_, y_train)\n",
    "    print(\"Take time: \", time.time() - t)\n",
    "\n",
    "    ypred_test = lgb.predict(X_test_)\n",
    "    dict_result[index] = ypred_test\n",
    "\n",
    "    result_valid_ = mean_squared_error(y_valid, lgb.predict(X_valid_), squared=False)\n",
    "    print(result_valid_)\n",
    "    print('-----------------')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ypred_test = np.mean([value for key, value in dict_result.items()], axis=0)\n",
    "pd.DataFrame(data = {\"{}\".format(target_): ypred_test, \"index\": test_index}).to_csv(\"submit_linear.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ridge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dict_result1 = {}\n",
    "\n",
    "year_valid = [2015, 2015, 2015, 2015, 2015, 2015, 2016, 2016, 2016, 2016]\n",
    "month_valid = [1, 3, 5, 7, 9, 11, 1, 3, 5, 7]\n",
    "\n",
    "for index in range(len(year_valid)):\n",
    "    t = time.time()\n",
    "\n",
    "    if index in dict_result1:\n",
    "        continue\n",
    "    y, m_s, m_e = year_valid[index], month_valid[index], month_valid[index] + 1\n",
    "\n",
    "    print(\"Training - {}\".format(index))\n",
    "    print(f'Test on month {m_s} and {m_e}, year {y}')\n",
    "\n",
    "    X_valid = df_train[(df_train[date_col].dt.year == y) & (df_train[date_col].dt.month >= m_s) & (df_train[date_col].dt.month <= m_e)]\n",
    "    X_train = df_train[~((df_train[date_col].dt.year == y) & (df_train[date_col].dt.month >= m_s) & (df_train[date_col].dt.month <= m_e))]\n",
    "\n",
    "    y_train = X_train[target_]\n",
    "    y_valid = X_valid[target_]\n",
    "\n",
    "    date = np.array(X_valid['startdate'])\n",
    "\n",
    "    test_index = df_test[\"index\"].values\n",
    "    X_test = df_test.copy()\n",
    "\n",
    "    cat_cols = [i for i in X_train.select_dtypes(include='object').columns if i != date_col]\n",
    "    X_train, listEncoder, df_mean = handle_feature_train_data(X_train, date_col, cat_cols.copy())\n",
    "    X_valid = handle_feature_test_data(X_valid, listEncoder, df_mean, columns_cat=cat_cols.copy())\n",
    "    X_test = handle_feature_test_data(X_test, listEncoder, df_mean, columns_cat=cat_cols.copy())\n",
    "\n",
    "    drop_ = [*drop_col, *[col for col in X_train.columns if 'Unnamed' in col]]\n",
    "    # drop_ = [col for col in X_train.columns if 'Unnamed' in col]\n",
    "    X_train = X_train.drop(columns=drop_)\n",
    "    X_valid = X_valid.drop(columns=drop_)\n",
    "    X_test = X_test.drop(columns=drop_)\n",
    "\n",
    "    X_train = X_train.drop(columns=target_)\n",
    "    X_valid = X_valid.drop(columns=target_)\n",
    "\n",
    "    reg1 = Ridge(alpha=1)\n",
    "    reg1.fit(X_train, y_train)\n",
    "\n",
    "    ypred_train1 = reg1.predict(X_train)\n",
    "    ypred_valid1 = reg1.predict(X_valid)\n",
    "\n",
    "    result_train = mean_squared_error(y_train, ypred_train1, squared=False)\n",
    "    result_valid = mean_squared_error(y_valid, ypred_valid1, squared=False)\n",
    "\n",
    "    ypred_test1 = reg1.predict(X_test)\n",
    "    dict_result1[index] = ypred_test1\n",
    "\n",
    "    print(\"Take time: \", time.time() - t)\n",
    "    print(\"Train_score: {}  Valid_score: {}\".format(result_train, result_valid))\n",
    "    print(\"-------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ypred_test = np.mean([value for key, value in dict_result.items()], axis=0)\n",
    "pd.DataFrame(data = {\"{}\".format(target_): ypred_test, \"index\": test_index}).to_csv(\"submit_ridge.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
